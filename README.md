# ETL_End_to_End


This project implements a complete **ETL (Extract, Transform, Load) pipeline** using **Python, MySQL, and GitHub Actions**.  
It automates the process of **collecting raw data**, **transforming it into structured formats**, and **ingesting it into a relational database** for further analysis (e.g., Power BI dashboards).


## ğŸ“‚ Project Structure

ETL_End_to_End/
â”‚
â”œâ”€â”€ config/ # Database config & schema files
â”‚ â”œâ”€â”€ db_config.json
â”‚ â””â”€â”€ schema.sql
â”‚
â”œâ”€â”€ data/ # Data layers
â”‚ â”œâ”€â”€ raw/ # Raw datasets (collected)
â”‚ â”œâ”€â”€ processed/ # Intermediate cleaned data
â”‚ â””â”€â”€ gold/ # Final transformed datasets (Excel/JSON)
â”‚
â”œâ”€â”€ src/ # Source code
â”‚ â”œâ”€â”€ collect/ # Data extraction scripts
â”‚ â”‚ â””â”€â”€ collect_data.py
â”‚ â”œâ”€â”€ transform/ # Data transformation scripts
â”‚ â”‚ â””â”€â”€ transform_data.py
â”‚ â””â”€â”€ ingest/ # Data ingestion scripts
â”‚ â””â”€â”€ ingest_data.py
â”‚
â”œâ”€â”€ .github/
â”‚ â””â”€â”€ workflows/
â”‚ â””â”€â”€ etl_pipeline.yml # GitHub Actions workflow
â”‚
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Project documentation


## âš¡ Pipeline Overview

The ETL pipeline is divided into **3 stages**:

1. **Collect**  
   - Fetches raw datasets (e.g., from Kaggle API).  
   - Saves to `data/raw/`.

2. **Transform**  
   - Cleans, preprocesses, and enriches raw data.  
   - Outputs structured datasets in **Excel** and **JSON** format.  
   - Saves to `data/gold/`.

3. **Ingest**  
   - Loads transformed data into a **MySQL database**.  
   - Uses schema validation and error handling.  

---

## ğŸ”„ CI/CD with GitHub Actions

The project uses **GitHub Actions** for automation:

- **Setup**: Create directories, install dependencies.  
- **Collect**: Run data extraction script.  
- **Transform**: Run cleaning & transformation.  
- **Ingest**: Load into MySQL.  
- **Artifacts**: Pass data files between jobs.  

---

## ğŸ› ï¸ Setup Instructions

### 1ï¸âƒ£ Clone Repo
```bash
git clone https://github.com/<your-username>/ETL_End_to_End.git
cd ETL_End_to_End
2ï¸âƒ£ Create Virtual Environment
bash
Copy code
python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows
3ï¸âƒ£ Install Dependencies
bash
Copy code
pip install -r requirements.txt
4ï¸âƒ£ Configure Database
Update config/db_config.json with your MySQL credentials:

json
Copy code
{
  "host": "localhost",
  "user": "root",
  "password": "your_password",
  "database": "etl_db"
}
Run schema:

bash
Copy code
mysql -u root -p etl_db < config/schema.sql
5ï¸âƒ£ Run Scripts Locally
bash
Copy code
# Collect raw data
python src/collect/collect_data.py

# Transform data
python src/transform/transform_data.py

# Ingest into MySQL
python src/ingest/ingest_data.py

ğŸ“Š Use Case
Demonstrates end-to-end data engineering (ETL pipelines, automation, DB integration).

Prepares data for Power BI / Tableau dashboards.

Real-world workflow showing CI/CD for data pipelines.

Great for portfolio / hiring showcase.

ğŸ“Œ Tech Stack
Python (pandas, openpyxl, mysql-connector)

MySQL for structured data storage

GitHub Actions for CI/CD automation

Power BI (optional) for visualization

ğŸ“ˆ Next Steps
Add data quality checks

Enable logging & monitoring

Deploy pipeline on cloud (AWS/GCP/Azure)

Integrate with orchestration tools (Airflow/Prefect)

ğŸ‘¨â€ğŸ’» Author: [Parnavi Chavan]
ğŸ“Œ Repo: ETL_End_to_End

